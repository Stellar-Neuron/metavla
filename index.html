<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="We propose MetaVLA, a Meta-Learning based co-training framework that addresses the inefficiencies and brittleness of current VLA post-training pipelines.">
  <meta property="og:title" content="MetaVLA: Unified Meta Co-Training for Efficient Embodied Adaptation"/>
  <meta property="og:description" content="We propose MetaVLA, a Meta-Learning based co-training framework that addresses the inefficiencies and brittleness of current VLA post-training pipelines."/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/images/MetaVLA-v8.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>

  <!-- <meta name="twitter:title" content="MetaVLA: Unified Meta Co-Training for Efficient Embodied Adaptation">
  <meta name="twitter:description" content="We propose MetaVLA, a Meta-Learning based co-training framework that addresses the inefficiencies and brittleness of current VLA post-training pipelines."> -->
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <!-- <meta name="twitter:image" content="static/images/MetaVLA-v8.png">
  <meta name="twitter:card" content="summary_large_image"> -->
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="Vision Language Models, Vision Language Action Models, Meta-Learning, VLM, VLA, generalization, embodied AI">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>MetaVLA: Unified Meta Co-Training for Efficient Embodied Adaptation</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon.ico">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  
  <style>
    /* Consistent font sizes while keeping original fonts */
    .title.is-1 {
      font-size: 3rem !important;
      font-weight: 600 !important;
    }
    
    .title.is-3 {
      font-size: 2rem !important;
      font-weight: 600 !important;
    }
    
    .subtitle.is-3 {
      font-size: 1.5rem !important;
      font-weight: 400 !important;
    }
    
    .subtitle {
      font-size: 1.125rem !important;
      font-weight: 400 !important;
      line-height: 1.6 !important;
    }
    
    .publication-authors {
      font-size: 1.125rem !important;
      font-weight: 400 !important;
    }
    
    .content p, .content li {
      font-size: 1.125rem !important;
      font-weight: 400 !important;
      line-height: 1.6 !important;
    }
    
    .button {
      font-size: 1rem !important;
      font-weight: 500 !important;
    }
    
    .footer .content p {
      font-size: 0.875rem !important;
    }
  </style>
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>

  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@4/tex-mml-chtml.js"></script>
</head>
<body>

<!-- Hero section with title and authors -->
<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">MetaVLA</h1>
          <h2 class="subtitle is-3 publication-subtitle">Unified Meta Co-Training for Efficient Embodied Adaptation</h2>
          
            <div class="is-size-5 publication-authors">
            <!-- Paper authors -->
            <span class="author-block"><a href="https://stellar-neuron.github.io/metavla/" target="_blank">Chen Li</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://stellar-neuron.github.io/metavla/" target="_blank">Zhantao Yang</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://stellar-neuron.github.io/metavla/" target="_blank">Han Zhang</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://stellar-neuron.github.io/metavla/" target="_blank">Fangyi Chen</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://stellar-neuron.github.io/metavla/" target="_blank">Chenchen Zhu</a><sup>2</sup>,</span>
            <span class="author-block"><a href="https://stellar-neuron.github.io/metavla/" target="_blank">Anudeepsekhar Bolimera</a><sup>1</sup>,</span>
            <span class="author-block"><a href="https://stellar-neuron.github.io/metavla/" target="_blank">Marios Savvides</a><sup>1</sup></span>
            </div>

            <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>Carnegie Mellon University</span>
            <span class="author-block"><sup>2</sup>Meta Reality Labs, USA</span>
            </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Arxiv PDF link -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2510.05580" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>

              <!-- ArXiv abstract Link -->
              <span class="link-block">
                <a href="https://arxiv.org/abs/2510.05580" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>

              <!-- Github link -->
              <span class="link-block">
                <a href="https://github.com/..." target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fab fa-github"></i>
                  </span>
                  <span>Code (Coming soon)</span>
                </a>
              </span>

              <!-- HuggingFace Dataset Link -->
              <!-- <span class="link-block">
                <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fa fa-database"></i>
                  </span>
                  <span>HF Datasets</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Vision–Language–Action (VLA) models show promise in embodied reasoning, yet remain far from <em>true generalists</em>—they often require task-specific fine-tuning, incur high compute costs, and generalize poorly to unseen tasks. We propose <b>MetaVLA</b>, a unified, backbone-agnostic post-training framework for efficient and scalable alignment. MetaVLA introduces <em>Context-Aware Meta Co-Training</em>, which consolidates diverse target tasks into a single fine-tuning stage while leveraging structurally diverse auxiliary tasks to improve in-domain generalization. Unlike naive multi-task SFT, MetaVLA integrates a lightweight meta-learning mechanism, Meta Action Reasoner (MAR)—derived from Attentive Neural Processes—to enable rapid adaptation from diverse contexts with minimal architectural change or inference overhead. On the LIBERO benchmark, MetaVLA with six auxiliary tasks outperforms OpenVLA by up to 8.0% on long-horizon tasks, reduces training steps from 240K to 75K, and cuts GPU time by ~76%. These results show that scalable, low-resource post-training is achievable—paving the way toward general-purpose embodied agents.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method overview image -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <img src="static/images/MetaVLA-v8.png" alt="MetaVLA framework overview" class="image"/>
        <h2 class="subtitle has-text-centered">
          <b>MetaVLA Architecture</b>. VLA backbone married with <em>Context-Aware Meta Co-Training</em> Framework, where the context memory bank is composed of both in-domain target tasks and out-of-domain auxiliary tasks.
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Motivation section -->
<!-- <section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Despite advances in new task adaptation, current VLAs are not yet <em>true generalists</em>—still far from fully out-of-the-box usability and reliant on alignment through post-training.
            Compounding this, post-training remains practically constrained by benchmarks with low per-task data. Current practice fine-tunes each downstream task independently, increasing overall training cost, hindering knowledge transfer across related tasks, and ultimately limiting success rate.
            These task-specific schedules are often brittle: many gradient steps are required before stably meaningful action sequences emerge, raising the risk of poor generalization and slowing adaptation to new task variants. For example, OpenVLA requires 240K training steps to fine-tune across all four LIBERO suites, while OpenVLA-OFT demands approximately 150K~500K steps, including both diffusion and non-diffusion parts. Long-horizon tasks such as LIBERO-Long further dominate the training schedule and often become the system bottleneck.
          </p>
          <p>
            While recent work has focused on expanding datasets and exploring backbone architecture or training protocol innovations during pretraining, we instead tackle it from an orthogonal perspective at the post-training stage.  Our experiments begin with a vanilla multi-task co-training setting: applying a standard SFT to a single model across related in-domain tasks (i.e., the four LIBERO suites). Indeed, we observe a reduction in total GPU training hours and improved success rates, which naturally motivates us to raise a question: can we introduce even more auxiliary tasks in the co-training to further boost VLA models? Sadly, we find that naively adding auxiliary tasks with greater domain diversity slows convergence and degrades performance. We attribute this surprise to the optimization instability arising from heterogeneous distributions, where misalignments in both the feature space (e.g., camera views) and action space (e.g., degrees of freedom) hinder the benefits of co-training.
          </p>
          <br>
        </div>
        <img src="static/images/MetaVLA-Fig1.2.png" alt="Stronger cross-task generalization with one single model." class="image"/>
        <h2 class="subtitle has-text-justified">
          <b>Stronger cross-task generalization with one single model.</b> OpenVLA requires training separate models for each task suite, resulting in higher training costs and poor cross-task performance. In contrast, MetaVLA achieves strong generalization across all four suites with a single unified model.
        </h2>
      </div>
    </div>
  </div>
</section> -->

<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified">
          <p>
            Despite progress in task adaptation, current VLAs are not true generalists—still reliant on costly, task-specific post-training. Fine-tuning per task limits transfer, raises training costs, and often requires many brittle gradient steps before stable behaviors emerge, slowing adaptation and risking poor generalization. For instance, OpenVLA needs 240K steps across LIBERO, while OpenVLA-OFT requires 150K–500K, with long-horizon tasks like LIBERO-Long becoming bottlenecks.
          </p>
          <p>
            While prior work expands datasets or innovates at pretraining, we instead target post-training. Starting with multi-task co-training (SFT on all four LIBERO suites), we reduce GPU hours and improve success. This raises a question: can auxiliary tasks further boost VLAs? Surprisingly, naive inclusion hurts—convergence slows and performance drops. We attribute this to optimization instability from heterogeneous distributions, where mismatches in feature (e.g., camera views) and action spaces (e.g., DoFs) offset co-training benefits.
          </p>
          <br>
        </div>
        <img src="static/images/MetaVLA-Fig1.2.png" alt="Stronger cross-task generalization with one single model." class="image"/>
        <h2 class="subtitle has-text-justified">
          <b>Stronger cross-task generalization with one single model.</b> OpenVLA requires training separate models for each task suite, resulting in higher training costs and poor cross-task performance. In contrast, MetaVLA achieves strong generalization across all four suites with a single unified model.
        </h2>
      </div>
    </div>
  </div>
</section>

<!-- Two analysis images side by side -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <div class="columns">
          <div class="column is-half has-text-centered">
            <img src="static/images/MetaVLA-Fig1.1.png" alt="Higher success rate with fewer training steps." class="image"/>
            <h2 class="subtitle has-text-justified">
              <b>Higher success rate with fewer training steps.</b> MetaVLA achieves a 4.4% higher average success rate while requiring 68.75% fewer training steps compared to the OpenVLA baseline on LIBERO benchmarks.
            </h2>
          </div>
          <div class="column is-half has-text-centered">
            <img src="static/images/convergence_comparison_1_bimanual_5_single_acc.png" alt="Faster convergence to higher accuracy across all target tasks." class="image"/>
            <h2 class="subtitle has-text-justified">
              <b>Faster convergence to higher accuracy across all target tasks.</b> Comparison of training accuracy between MetaVLA and a baseline multi-task SFT when auxiliary tasks are added. MetaVLA consistently converges to higher accuracy across all LIBERO suites, while the baseline underperforms throughout training.
            </h2>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Paper contributions -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Contributions</h2>
        <div class="content has-text-justified">
          <ul>
            <li>We investigate an underexplored direction: improving post-training efficiency and generalization ability through incorporating diverse auxiliary tasks with negligible optimization overhead.</li>
            <li>We propose MetaVLA, a suite of plug-in module and training recipes that enables fast and scalable adaptation with strong generalization. MetaVLA is engineering-friendly and agnostic to backbone architectures and underlying training pipelines.</li>
            <li>We conduct comprehensive experiments to show that MetaVLA delivers superior performance with significant efficiency gains by reducing model count and GPU training hours, while preserving fast inference.</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Method section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method</h2>

        <!-- <h2 class="subtitle has-text-justified">
          <p>
            We propose MAR, a compact Attentive Neural Process–based module integrated into the Llama-2 action decoder to improve convergence and generalization in low-data task adaptation. Following the ANP formulation, MAR models the conditional distribution of target actions given context examples by first applying self-attention over context pairs $(x_{Ci}, y_{Ci})$ to extract global priors, then fusing these with target queries $x_T$ via cross-attention to obtain task-aware representations:
          </p>
        </h2>

        <img src="static/images/eq1.png" alt="Impact of TopoAug dataset and training methods" class="image" style="transform: scale(0.7);"/>

        <h2 class="subtitle has-text-justified">
          <p>
            where $\mathbf{r}{Ci}$ and $\mathbf{s}{Ci}$ are per-context representations aggregated through self-attention, $\mathbf{r}_T$ is the cross-attention output, $\mathbf{\Bar{s}}_C$ is the mean context embedding, and $z$ is a latent drawn from $q(z|\mathbf{\Bar{s}}_C)$. Training leverages reparameterization with both context and target representations, optimizing a variational bound:
          </p>
        </h2>
        
        <img src="static/images/eq2.png" alt="Quantitative evaluation results" class="image"/>

        <h2 class="subtitle has-text-justified">
          <p>
            which regularizes target reconstruction against the context distribution. Unlike standard ANP, MAR is integrated into a pretrained Llama-2 backbone, where its stochastic and deterministic latent vectors are concatenated with Llama hidden states before decoding, enabling end-to-end action prediction.
          </p>
        </h2>

        <br><br> -->

        <h2 class="subtitle has-text-justified">
          <p>
            We enhance MetaVLA with auxiliary tasks by adding GR00T, which was unseen during OpenVLA pretraining, that balances LIBERO's domain relevance with structural diversity.
          </p>
        </h2>

        <img src="static/images/MetaVLA-Data-v3.png" alt="Impact of TopoAug dataset and training methods" class="image"/>
        <h2 class="subtitle has-text-justified">
          <b>Comparison between auxiliary tasks and LIBERO evaluation benchmark.</b> LIBERO tasks use third-person front-view images and 7-DoF actions for a single-arm robot. In contrast, our auxiliary data from GR00T introduces variation through side-view observations and a two-arm robot with 14-DoF actions. MetaVLA benefits from this data diversity, while OpenVLA struggles with the domain mismatch.
        </h2>

        <br><br>

      </div>
    </div>
  </div>
</section>

<!-- Experiments section -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Experiments</h2>
        
        <img src="static/images/main_table.png" alt="Quantitative evaluation results" class="image"/>
        <h2 class="subtitle has-text-justified">
          <b>Success rate comparison with prior methods.</b> All MetaVLA variants are single models trained for 75K steps. <em>MetaVLA (ours)</em> uses only LIBERO suites in the context bank without the stochastic module, while <em>MetaVLA+Stochastic (ours)</em> includes it. <em>SFT-4LIBERO</em> is a single-model baseline trained with vanilla multi-task SFT across all suites. <em>OpenVLA (top)</em> comprises four Hugging Face models fine-tuned separately on LIBERO using the OpenVLA-7B backbone, totaling roughly 240K steps. MetaVLA with six auxiliary tasks surpasses OpenVLA by 4.4% and SFT-4LIBERO by 3.1% on average, with even larger gains on LIBERO-Long (8.0% and 5.1%, respectively).
        </h2>

        <br><br>

        <img src="static/images/contexts_comparison.combined.png" alt="Comparison of accuracy and token length" class="image"/>
        <h2 class="subtitle has-text-justified">
          <b>Left: Per-suite LIBERO success rate across varying context batch sizes.</b> <em>OpenVLA</em> refers to the four Hugging Face baseline models, each fine-tuned individually on LIBERO using the OpenVLA-7B backbone, while <em>SFT-4LIBERO</em> is a single-model baseline trained with vanilla multi-task SFT across all suites. For each suite, success rate increases monotonically with context batch size. <b>Right: Average success rate across LIBERO suites with varying context batch sizes.</b> <em>OpenVLA</em> denotes the four Hugging Face models baselines fine-tuned individually on LIBERO with the OpenVLA-7B backbone, while <em>SFT-4LIBERO</em> is a single-model baseline trained with vanilla multi-task SFT across all suites. b<sub>c</sub> indicates the context batch size. Larger context batches consistently yield higher average success rates.
        </h2>

        <br><br>

        <img src="static/images/single.png" alt="Comparison of accuracy and token length" class="image"/>
        <h2 class="subtitle has-text-justified">
          <b>MetaVLA-EACH: Per-suite success rates across LIBERO.</b> <em>OpenVLA</em> denotes the four baseline models fine-tuned separately for each LIBERO suite, released on Hugging Face and trained for 240K total steps. <em>OpenVLA-120K</em> follows the same setup but with 120K steps. <em>MetaVLA-EACH-120K</em> and <em>MetaVLA-EACH-240K</em> are our models trained separately per suite for 120K and 240K steps, respectively, without co-training. Thanks to the <em>MAR</em> design, all MetaVLA-EACH variants outperform their OpenVLA counterparts with fewer steps. For Goal and Long, performance continues to improve at 240K steps, indicating stronger learning potential.
        </h2>

        <br><br>
      </div>
    </div>
  </div>
</section>

<!-- Conclusion -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Conclusion</h2>
        <div class="content has-text-justified">
          <p>
            We presented <b>MetaVLA</b>, a framework that addresses the inefficiencies and brittleness of current VLA post-training pipelines. By introducing <em>Context-Aware Meta Co-Training</em>, MetaVLA integrates auxiliary tasks without destabilizing optimization, achieving superior convergence speed, efficiency, and generalization. MetaVLA is lightweight, plug-and-play, and backbone-agnostic, making it easy to extend beyond supervised fine-tuning to reinforcement learning or hybrid pipelines. Empirical results on LIBERO show consistent gains over both per-task fine-tuning and naive multi-task SFT, while significantly reducing training cost and model count. Looking forward, we envision extending MetaVLA to broader backbones and wider benchmarks, incorporating web-scale multimodal data, and deploying on real robots. We hope this work inspires future research toward efficient, scalable, and truly generalist embodied VLA systems.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- BibTeX citation -->
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
      @misc{li2025metavlaunifiedmetacotraining,
            title={MetaVLA: Unified Meta Co-training For Efficient Embodied Adaption}, 
            author={Chen Li and Zhantao Yang and Han Zhang and Fangyi Chen and Chenchen Zhu and Anudeepsekhar Bolimera and Marios Savvides},
            year={2025},
            eprint={2510.05580},
            archivePrefix={arXiv},
            primaryClass={cs.AI},
            url={https://arxiv.org/abs/2510.05580}, 
      }
    </code></pre>
  </div>
</section>

<!-- Footer -->
<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
